{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‑of‑the‑art inference‑time scaling methods such as *Chain‑of‑Thought* prompting and *Tree‑of‑Thoughts*, and briefly explore high-levels of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‑time scaling methods: **zero‑shot / few‑shot CoT, self‑consistency, sequential decoding, tree‑of‑thoughts**  \n",
    "* Gain intuition for **training** reasoning‑capable models following **STaR** approach \n",
    "* Build a minimal **deep‑research agent** that combines step‑by‑step reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "1. Environment setup  \n",
    "2. Inference‑time scaling  \n",
    "   2.1 Few‑shot & zero‑shot CoT  \n",
    "   2.2 Self‑consistency\n",
    "   2.3 Sequential revisions  \n",
    "   2.4 Tree‑of‑Thought\n",
    "3. STaR for training models for reasoning  \n",
    "4. Deep-research agent  \n",
    "5. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e480f76",
   "metadata": {},
   "source": [
    "# 1‑ Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Once this is done, you can select \"deep_research\" from the Kernel → Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project we use the `llama3.2:3b` and `deepseek-r1:8b` models. You can try other smaller or larger reasoning LLMs such as `qwen2.5:3b-instruct` or `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:8b\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull qwen2.5:3b-instruct\n",
    "# ollama pull phi4-mini\n",
    "\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‑ Inference‑time scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model’s weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we’ll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 2.1: Few‑Shot CoT\n",
    "Few-shot prompting helps a model reason by showing one or multiple examples before asking a new question. By observing the pattern of reasoning and final answers, the model learns how to structure its own reasoning process on the new input.\n",
    "\n",
    "In this exercise, you will create a prompt that includes a few example Q&A pairs demonstrating step-by-step reasoning. Then, you will feed a new question and see the model’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's reasoning and answer:\n",
      "Let's break down the problem of filling the swimming pool. I'll solve it step by step.\n",
      "\n",
      "Step 1: Calculate rate of each pipe\n",
      "- Pipe A fills the pool in 6 hours, so its rate is 1/6 of the pool per hour.\n",
      "- Pipe B fills the pool in 4 hours, so its rate is 1/4 of the pool per hour.\n",
      "\n",
      "Step 2: Calculate combined rate without drain\n",
      "- Combined rate = Rate of Pipe A + Rate of Pipe B\n",
      "= 1/6 + 1/4\n",
      "To add these fractions, we need a common denominator, which is 12.\n",
      "= (2/12) + (3/12)\n",
      "= 5/12\n",
      "\n",
      "Step 3: Calculate rate of drain\n",
      "- The drain empties the full pool in 12 hours, so its rate is 1/12 of the pool per hour.\n",
      "\n",
      "Step 4: Calculate combined rate with drain\n",
      "- Combined rate = Rate of Pipe A + Rate of Pipe B - Rate of Drain\n",
      "= 5/12 - 1/12\n",
      "= 4/12\n",
      "\n",
      "Step 5: Simplify combined rate\n",
      "- The fraction 4/12 can be simplified by dividing both numerator and denominator by their greatest common divisor, which is 4.\n",
      "= 1/3\n",
      "\n",
      "Step 6: Calculate time to fill the pool\n",
      "- Time = Rate / Combined Rate\n",
      "Since the rate is expressed as a fraction (1/3), we need to invert it and multiply instead of divide:\n",
      "- Time = 1 / (Rate) \n",
      "= 3 hours\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Write a few examples showing reasoning steps\n",
    "# Step 2: Write your new question\n",
    "# Step 3: Concatenate examples + new question into a single prompt\n",
    "# Step 4: Call your Ollama or OpenAI client to get a response from llama3.2:3b # e.g., client.chat.completions.create(...)\n",
    "# Step 5: Print the final answer\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client pointing to Ollama running on the host\n",
    "client = OpenAI(\n",
    "    api_key=\"ollama\",\n",
    "    base_url=\"http://host.docker.internal:11434/v1\"\n",
    ")\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "# Few-shot examples with explicit reasoning steps\n",
    "few_shot_examples = \"\"\"\n",
    "Q: A farmer has 3 chickens. Each chicken lays 2 eggs per day. After 4 days, the farmer sells half of the eggs. Then each remaining egg hatches into a chick that will lay 2 eggs per day. How many eggs will the farmer have after 2 more days?\n",
    "\n",
    "A: Let me work through this step by step:\n",
    "Step 1: Calculate initial eggs laid\n",
    "- 3 chickens x 2 eggs/day x 4 days = 24 eggs\n",
    "\n",
    "Step 2: Calculate eggs after selling half\n",
    "- Eggs sold: 24 ÷ 2 = 12 eggs\n",
    "- Eggs remaining: 24 - 12 = 12 eggs\n",
    "\n",
    "Step 3: Calculate new chickens from hatched eggs\n",
    "- 12 eggs hatch into 12 new chicks\n",
    "- Total chickens now: 3 + 12 = 15 chickens\n",
    "\n",
    "Step 4: Calculate eggs laid by all chickens over 2 days\n",
    "- 15 chickens x 2 eggs/day x 2 days = 60 eggs\n",
    "\n",
    "Final answer: 60 eggs\n",
    "\n",
    "---\n",
    "\n",
    "Q: A library charges $2 for the first day a book is late, and the fine doubles each additional day. If someone returns a book 5 days late, how much do they owe?\n",
    "\n",
    "A: Let me solve this step by step:\n",
    "Step 1: Day 1 fine\n",
    "- First day: $2\n",
    "\n",
    "Step 2: Day 2 fine (doubles)\n",
    "- Second day: $2 x 2 = $4\n",
    "\n",
    "Step 3: Day 3 fine (doubles again)\n",
    "- Third day: $4 x 2 = $8\n",
    "\n",
    "Step 4: Day 4 fine\n",
    "- Fourth day: $8 x 2 = $16\n",
    "\n",
    "Step 5: Day 5 fine\n",
    "- Fifth day: $16 x 2 = $32\n",
    "\n",
    "Step 6: Total fine\n",
    "- $2 + $4 + $8 + $16 + $32 = $62\n",
    "\n",
    "Final answer: $62\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# A challenging question that requires multi-step reasoning\n",
    "new_question = \"\"\"\n",
    "Q: A swimming pool can be filled by Pipe A in 6 hours and by Pipe B in 4 hours. However, there's a drain that can empty the full pool in 12 hours. If all three (both pipes and the drain) are opened simultaneously when the pool is empty, how long will it take to fill the pool?\n",
    "\n",
    "A: Let me work through this step by step:\n",
    "\"\"\"\n",
    "\n",
    "# This problem is harder than simple arithmetic because it involves:\n",
    "# \n",
    "# Converting time-to-complete into rates (1/6, 1/4, 1/12 pools per hour)\n",
    "# Understanding that the drain works against the pipes\n",
    "# Combining rates: 1/6 + 1/4 - 1/12 = net rate\n",
    "# 2/12 + 3/12 - 1/12 = 4/12 = 1/3 pools per hour\n",
    "# Finding the time to fill the pool at the net rate (3 hours)\n",
    "\n",
    "# Concatenate examples and question\n",
    "prompt = few_shot_examples + new_question\n",
    "\n",
    "# Call the model\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Model's reasoning and answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "### (Optional) Few-shot CoT on GPT2\n",
    "GPT-2 is a pre-trained language model without instruction tuning. It continues text rather than answering questions. In this section, you'll try the exact same CoT pattern on GPT-2 and observe what happens. The goal is to test whether few-shot CoT alone can elicit structured reasoning from a non-chat LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22cf6f597244b809dec63ba9d1ea376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ecc6bbddee4245b9dc5beda56a2bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feff8994d244f96a6e69ed9567dc58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a33d24e46348758c0399dd66fd9d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74d4a64dc604e2dba0302f3084f662b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6846ac4815140e18608b09298194fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf3f894f85a404db5d594343218a542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 1: GPT-2 WITHOUT Chain-of-Thought (Question Only)\n",
      "================================================================================\n",
      "\n",
      "Generated text (without CoT):\n",
      "\n",
      "Q: A swimming pool can be filled by Pipe A in 6 hours and by Pipe B in 4 hours. However, there's a drain that can empty the full pool in 12 hours. If all three (both pipes and the drain) are opened simultaneously when the pool is empty, how long will it take to fill the pool?\n",
      "\n",
      "A: Let me work through this step by step:\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Empty a pipe\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "Fill a pipe with water\n",
      "\n",
      "================================================================================\n",
      "TEST 2: GPT-2 WITH Chain-of-Thought (Examples + Question)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text (with CoT):\n",
      "\n",
      "Step 1: Day 1 fine\n",
      "\n",
      "- First day: $2\n",
      "\n",
      "- Second day: $2 x 2 = $4\n",
      "\n",
      "- Third day: $2 x 2 = $16\n",
      "\n",
      "Step 2: Day 3 fine\n",
      "\n",
      "- Fourth day: $8 x 2 = $16\n",
      "\n",
      "Step 3: Day 3 fine (doubles again)\n",
      "\n",
      "- Fifth day: $16 x 2 = $32\n",
      "\n",
      "Step 4: Day 4 fine\n",
      "\n",
      "- Fifth day: $32 x 2 = $32\n",
      "\n",
      "Step 5: Day 5 fine (doubles again)\n",
      "\n",
      "- Sixth day: $16 x 2 = $32\n",
      "\n",
      "Step 6: Day 6 fine\n",
      "\n",
      "- Seventh day: $32 x 2 = $32\n",
      "\n",
      "Step 6: Total fine\n",
      "\n",
      "---\n",
      "\n",
      "Q: A carpenter can build a house, and yet there are no roof or cabinets, and the house still sits on the ground. How long\n",
      "\n",
      "================================================================================\n",
      "TEST 3: Different Decoding Strategies (with CoT)\n",
      "================================================================================\n",
      "\n",
      "--- Greedy Decoding ---\n",
      "\n",
      "Step 1: Pipe A empties the full pool in 12 hours\n",
      "\n",
      "Step 2: Pipe B empties the full pool in 4 hours\n",
      "\n",
      "Step 3: Pipe A empties the full pool in 8 hours\n",
      "\n",
      "Step 4: Pipe B empties the full pool in 12 hours\n",
      "\n",
      "Final answer: 12 hours\n",
      "\n",
      "---\n",
      "\n",
      "Q: A water heater can be used to heat a house. How much heat does it cost to heat a house?\n",
      "\n",
      "A: Let me solve this step by step:\n",
      "\n",
      "Step 1: Water heater\n",
      "\n",
      "- 1 gallon of water\n",
      "\n",
      "- 1 gallon of water\n",
      "\n",
      "- 1 gallon of water\n",
      "\n",
      "- 1 gallon of water\n",
      "\n",
      "- 1 gallon of water\n",
      "\n",
      "\n",
      "--- Top-K Sampling (k=50) ---\n",
      "\n",
      "Step 1: The pool is filled with water.\n",
      "\n",
      "Step 2: The end of the pool can be seen as a large green dot. The green dot is the same as the bottom of the blue dot (the same as the bottom of the red dot).\n",
      "\n",
      "Step 3: Pipe A closes the water and pipe B closes the water.\n",
      "\n",
      "Step 4: Pipe B closes the water and pipe A closes the water.\n",
      "\n",
      "Step 5: Pipe A closes the water and pipe B closes the water.\n",
      "\n",
      "Step 6: Pipe A closes the water and pipe B closes the water.\n",
      "\n",
      "Final answer: Pipe A closes the water and pipe B closes the water.\n",
      "\n",
      "---\n",
      "\n",
      "Q: A carpenter uses\n",
      "\n",
      "--- Top-P Sampling (p=0.9) ---\n",
      "\n",
      "Step 1: Add Pipe A to Pipe B\n",
      "\n",
      "- 1 pump\n",
      "\n",
      "- 1 pot\n",
      "\n",
      "- 1 bucket\n",
      "\n",
      "- 1 bucket of water\n",
      "\n",
      "- 1 bucket of salt\n",
      "\n",
      "- 1 bucket of water (optional)\n",
      "\n",
      "Step 2: Add Pipe B to Pipe A\n",
      "\n",
      "- 1 pump\n",
      "\n",
      "- 1 pot\n",
      "\n",
      "- 1 bucket\n",
      "\n",
      "- 1 bucket of water\n",
      "\n",
      "- 1 bucket of salt\n",
      "\n",
      "- 1 bucket of water (optional)\n",
      "\n",
      "Step 3: Add Pipe B to Pipe A\n",
      "\n",
      "- 1 pump\n",
      "\n",
      "- 1 pot\n",
      "\n",
      "- 1 bucket\n",
      "\n",
      "- 1 bucket of water\n",
      "\n",
      "- 1 bucket of salt\n",
      "\n",
      "- 1 bucket of water (optional)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Without CoT length: 1006 chars\n",
      "With CoT length: 2087 chars\n",
      "New generation length: 512 chars\n",
      "\n",
      "Note: GPT-2 is a completion model, not an instruction-tuned chat model.\n",
      "It may not follow the step-by-step format perfectly, but CoT examples\n",
      "can still influence the continuation pattern.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load GPT-2 text-generation from huggingface (https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
    "# Step 2: Write 1–2 few-shot reasoning examples (short, explicit steps + final answer in your own unique format)\n",
    "# Step 3: Append a new test question after the examples to form one prompt string\n",
    "# Step 4: Generate 1–3 completions with different decoding settings (e.g., greedy vs. top-k)\n",
    "# Step 5: Print raw outputs; check if steps are followed and if the final answer is correct\n",
    "\n",
    "# Reuse the same examples and question from the original code\n",
    "few_shot_examples = \"\"\"\n",
    "Q: A farmer has 3 chickens. Each chicken lays 2 eggs per day. After 4 days, the farmer sells half of the eggs. Then each remaining egg hatches into a chick that will lay 2 eggs per day. How many eggs will the farmer have after 2 more days?\n",
    "\n",
    "A: Let me work through this step by step:\n",
    "Step 1: Calculate initial eggs laid\n",
    "- 3 chickens x 2 eggs/day x 4 days = 24 eggs\n",
    "\n",
    "Step 2: Calculate eggs after selling half\n",
    "- Eggs sold: 24 ÷ 2 = 12 eggs\n",
    "- Eggs remaining: 24 - 12 = 12 eggs\n",
    "\n",
    "Step 3: Calculate new chickens from hatched eggs\n",
    "- 12 eggs hatch into 12 new chicks\n",
    "- Total chickens now: 3 + 12 = 15 chickens\n",
    "\n",
    "Step 4: Calculate eggs laid by all chickens over 2 days\n",
    "- 15 chickens x 2 eggs/day x 2 days = 60 eggs\n",
    "\n",
    "Final answer: 60 eggs\n",
    "\n",
    "---\n",
    "\n",
    "Q: A library charges $2 for the first day a book is late, and the fine doubles each additional day. If someone returns a book 5 days late, how much do they owe?\n",
    "\n",
    "A: Let me solve this step by step:\n",
    "Step 1: Day 1 fine\n",
    "- First day: $2\n",
    "\n",
    "Step 2: Day 2 fine (doubles)\n",
    "- Second day: $2 x 2 = $4\n",
    "\n",
    "Step 3: Day 3 fine (doubles again)\n",
    "- Third day: $4 x 2 = $8\n",
    "\n",
    "Step 4: Day 4 fine\n",
    "- Fourth day: $8 x 2 = $16\n",
    "\n",
    "Step 5: Day 5 fine\n",
    "- Fifth day: $16 x 2 = $32\n",
    "\n",
    "Step 6: Total fine\n",
    "- $2 + $4 + $8 + $16 + $32 = $62\n",
    "\n",
    "Final answer: $62\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "new_question = \"\"\"\n",
    "Q: A swimming pool can be filled by Pipe A in 6 hours and by Pipe B in 4 hours. However, there's a drain that can empty the full pool in 12 hours. If all three (both pipes and the drain) are opened simultaneously when the pool is empty, how long will it take to fill the pool?\n",
    "\n",
    "A: Let me work through this step by step:\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading GPT-2 model...\")\n",
    "# Load GPT-2 text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 1: GPT-2 WITHOUT Chain-of-Thought (Question Only)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate without CoT examples\n",
    "output_no_cot = generator(\n",
    "    new_question,\n",
    "    max_new_tokens=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated text (without CoT):\")\n",
    "print(output_no_cot[0][\"generated_text\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: GPT-2 WITH Chain-of-Thought (Examples + Question)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate with CoT examples\n",
    "prompt_with_cot = few_shot_examples + new_question\n",
    "\n",
    "output_with_cot = generator(\n",
    "    prompt_with_cot,\n",
    "    max_new_tokens=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated text (with CoT):\")\n",
    "# Only print the new generation after the prompt\n",
    "generated_part = output_with_cot[0][\"generated_text\"][len(prompt_with_cot):]\n",
    "print(generated_part)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: Different Decoding Strategies (with CoT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Greedy decoding\n",
    "print(\"\\n--- Greedy Decoding ---\")\n",
    "greedy_output = generator(\n",
    "    prompt_with_cot,\n",
    "    max_new_tokens=150,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=False,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "print(greedy_output[0][\"generated_text\"][len(prompt_with_cot):])\n",
    "\n",
    "# Top-k sampling\n",
    "print(\"\\n--- Top-K Sampling (k=50) ---\")\n",
    "topk_output = generator(\n",
    "    prompt_with_cot,\n",
    "    max_new_tokens=150,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "print(topk_output[0][\"generated_text\"][len(prompt_with_cot):])\n",
    "\n",
    "# Top-p (nucleus) sampling\n",
    "print(\"\\n--- Top-P Sampling (p=0.9) ---\")\n",
    "topp_output = generator(\n",
    "    prompt_with_cot,\n",
    "    max_new_tokens=150,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "print(topp_output[0][\"generated_text\"][len(prompt_with_cot):])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Without CoT length: {len(output_no_cot[0]['generated_text'])} chars\")\n",
    "print(f\"With CoT length: {len(output_with_cot[0]['generated_text'])} chars\")\n",
    "print(f\"New generation length: {len(generated_part)} chars\")\n",
    "print(\"\\nNote: GPT-2 is a completion model, not an instruction-tuned chat model.\")\n",
    "print(\"It may not follow the step-by-step format perfectly, but CoT examples\")\n",
    "print(\"can still influence the continuation pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 2.2: Zero‑Shot Chain‑of‑Thought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as “Let’s think step by step.” This simple phrase often activates the model’s latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's reasoning and answer:\n",
      "To solve this problem, we need to find the combined rate at which Pipe A fills the pool, Pipe B fills the pool, and the drain empties the pool. Then, we can calculate the net rate of filling the pool.\n",
      "\n",
      "Step 1: Find the individual rates of each pipe:\n",
      "\n",
      "- Pipe A's rate = 1 pool / 6 hours = 1/6 pool per hour\n",
      "- Pipe B's rate = 1 pool / 4 hours = 1/4 pool per hour\n",
      "\n",
      "Step 2: Find the drain's rate (negative, because it empties the pool):\n",
      "\n",
      "- Drain's rate = -1 pool / 12 hours = -1/12 pool per hour (note the negative sign)\n",
      "\n",
      "Step 3: Calculate the combined rate of filling the pool:\n",
      "\n",
      "Combined rate = Pipe A's rate + Pipe B's rate + Drain's rate\n",
      "= 1/6 pool/hour + 1/4 pool/hour - 1/12 pool/hour\n",
      "\n",
      "To add these fractions, we need a common denominator (12). We'll convert each fraction accordingly:\n",
      "- 1/6 = 2/12\n",
      "- 1/4 = 3/12\n",
      "- -1/12 remains the same\n",
      "\n",
      "Now, let's calculate the combined rate:\n",
      "\n",
      "Combined rate = 2/12 + 3/12 - 1/12\n",
      "= (2+3-1)/12 pool/hour\n",
      "= 4/12 pool/hour\n",
      "= 1/3 pool per hour\n",
      "\n",
      "Step 4: To find the time to fill the pool, we take the reciprocal of the combined rate:\n",
      "\n",
      "Time = 1 / Combined rate\n",
      "= 1 / (1/3)\n",
      "= 3 hours\n",
      "\n",
      "Therefore, it will take 3 hours to fill the pool when all three are opened simultaneously.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Write the question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 2: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"ollama\",\n",
    "    base_url=\"http://host.docker.internal:11434/v1\"\n",
    ")\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "A swimming pool can be filled by Pipe A in 6 hours and by Pipe B in 4 hours. However, there's a drain that can empty the full pool in 12 hours. If all three (both pipes and the drain) are opened simultaneously when the pool is empty, how long will it take to fill the pool?\n",
    "\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Model's reasoning and answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 2.3 Self‑Consistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 10 reasoning chains...\n",
      "  Chain 1: Yes! 12 squared equals 144, so we can confirm\n",
      "  Chain 2: The square root of 144 is 12\n",
      "  Chain 3: square root of the number 144 has two factors for both positive and a Negative value\n",
      "  Chain 4: √144 = ±12\n",
      "  Chain 5: Is that what you were expecting?\n",
      "  Chain 6: is:\n",
      "  Chain 7: And there you have it. The square root of 144 is indeed 12.\n",
      "  Chain 8: Therefore, the square root of 144 is 12.\n",
      "  Chain 9: is:\n",
      "  Chain 10: So, the square root of 144 is indeed 12!\n",
      "Votes: Counter({'is:': 2, 'Yes! 12 squared equals 144, so we can confirm': 1, 'The square root of 144 is 12': 1, 'square root of the number 144 has two factors for both positive and a Negative value': 1, '√144 = ±12': 1, 'Is that what you were expecting?': 1, 'And there you have it. The square root of 144 is indeed 12.': 1, 'Therefore, the square root of 144 is 12.': 1, 'So, the square root of 144 is indeed 12!': 1})\n",
      "Chosen answer: is:\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re, collections\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://host.docker.internal:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def cot_answer(question, temperature=1.0):\n",
    "    # Generate a step-by-step reasoning chain for the given question and extract the final answer.\n",
    "    prompt = f\"{question}\\n\\nLet's think step by step.\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    full_response = response.choices[0].message.content\n",
    "    \n",
    "    # Extract final answer - look for common patterns\n",
    "    answer_match = re.search(r'(?:final answer|answer|therefore|thus|so)[:\\s]+([^\\n.]+)', full_response, re.IGNORECASE)\n",
    "    if answer_match:\n",
    "        return answer_match.group(1).strip()\n",
    "    \n",
    "    # Fallback: return last line\n",
    "    return full_response.strip().split('\\n')[-1].strip()\n",
    "\n",
    "def self_consistent(question, n=10):\n",
    "    # Run multiple reasoning chains and select the most frequent final answer by majority voting.\n",
    "    answers = []\n",
    "    \n",
    "    print(f\"\\nGenerating {n} reasoning chains...\")\n",
    "    for i in range(n):\n",
    "        answer = cot_answer(question, temperature=1.0)\n",
    "        answers.append(answer)\n",
    "        print(f\"  Chain {i+1}: {answer}\")\n",
    "    \n",
    "    # Count frequency of each answer\n",
    "    counter = collections.Counter(answers)\n",
    "    \n",
    "    # Get most common answer\n",
    "    winner = counter.most_common(1)[0][0]\n",
    "    \n",
    "    return winner, counter\n",
    "\n",
    "\n",
    "question = \"What is the square root of 144?\"\n",
    "winner, counter = self_consistent(question)\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 2.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SEQUENTIAL REVISION TEST\n",
      "================================================================================\n",
      "Expected answer: 3 hours\n",
      "Correct reasoning: 1/6 + 1/4 - 1/12 = 2/12 + 3/12 - 1/12 = 4/12 = 1/3 pool/hour → 3 hours\n",
      "\n",
      "================================================================================\n",
      "DRAFT 1 (Initial Answer)\n",
      "================================================================================\n",
      "To solve this problem, let's break down the process into steps:\n",
      "\n",
      "**Step 1: Calculate the rate of filling for each pipe and the drain**\n",
      "\n",
      " Pipe A fills the pool in 6 hours, so its rate is:\n",
      "Rate of Pipe A = 1 pool / 6 hours\n",
      "= 1/6 pool per hour\n",
      "\n",
      "Pipe B fills the pool in 4 hours, so its rate is:\n",
      "Rate of Pipe B = 1 pool / 4 hours\n",
      "= 1/4 pool per hour\n",
      "\n",
      "The drain empties the pool in 12 hours, so its rate is:\n",
      "Rate of Drain = -1 pool / 12 hours (negative sign indicates emptying)\n",
      "\n",
      "**Step 2: Calculate the combined rate**\n",
      "\n",
      "When all three pipes and the drain are opened simultaneously, their rates add up. To find the combined rate, we need to subtract the rate of the drain from the sum of the rates of Pipe A and Pipe B:\n",
      "Combined Rate = Rate of Pipe A + Rate of Pipe B - Rate of Drain\n",
      "= (1/6) + (1/4) - (-1/12)\n",
      "To add these fractions, we need a common denominator. The least common multiple (LCM) of 6, 4, and 12 is 12.\n",
      "\n",
      "Combined Rate = (2/12) + (3/12) + (1/12)\n",
      "= 6/12 pool per hour\n",
      "\n",
      "**Step 3: Simplify the combined rate**\n",
      "\n",
      "The combined rate can be simplified by dividing both numerator and denominator by their greatest common divisor, which is 6:\n",
      "Combined Rate = (6/12) / 6\n",
      "= 1/2 pool per hour\n",
      "\n",
      "**Step 4: Calculate the time to fill the pool**\n",
      "\n",
      "Since the combined rate is 1/2 pool per hour, it will take half an hour to fill half the pool. To find the total time to fill the entire pool, we can multiply this by 2:\n",
      "Time = (1/2) x 2\n",
      "= 1 hour\n",
      "\n",
      "Therefore, when all three pipes and the drain are opened simultaneously, it will take **1 hour** to fill the swimming pool.\n",
      "\n",
      "Note: This calculation assumes that the rates of Pipe A and Pipe B remain constant over time. In reality, the flow rate might not be uniform due to various factors like pipe size, water pressure, etc. However, this simplification provides a reasonable estimate in this scenario.\n",
      "\n",
      "Extracted answer: 6 hours\n",
      "\n",
      "================================================================================\n",
      "DRAFT 2 (Revision 1)\n",
      "================================================================================\n",
      "**Solving the Problem: Filling a Swimming Pool with Multiple Pipes and a Drain**\n",
      "\n",
      "Let's break down the problem step-by-step to find the time it takes to fill the pool when all three pipes (Pipe A, Pipe B) and the drain are opened simultaneously.\n",
      "\n",
      "### Step 1: Calculate the rate of filling for each pipe and the drain\n",
      "\n",
      "- **Pipe A**: Fills the pool in 6 hours. Its rate is:\n",
      "\\[ \\text{Rate of Pipe A} = \\frac{1}{6} \\text{ pool per hour} \\]\n",
      "\n",
      "- **Pipe B**: Fills the pool in 4 hours. Its rate is:\n",
      "\\[ \\text{Rate of Pipe B} = \\frac{1}{4} \\text{ pool per hour} \\]\n",
      "\n",
      "- **The Drain**: Empties the pool in 12 hours. Its rate is:\n",
      "\\[ \\text{Rate of Drain} = -\\frac{1}{12} \\text{ pool per hour} \\]\n",
      "Note that the negative sign indicates emptying.\n",
      "\n",
      "### Step 2: Calculate the combined rate\n",
      "\n",
      "To find the combined rate when all three pipes and the drain are opened, we add their rates together. However, because the drain empties the pool, its rate is subtracted from the sum of the rates of Pipe A and Pipe B:\n",
      "\n",
      "\\[ \\text{Combined Rate} = \\text{Rate of Pipe A} + \\text{Rate of Pipe B} - \\text{Rate of Drain} \\]\n",
      "\\[ \\text{Combined Rate} = \\frac{1}{6} + \\frac{1}{4} - (-\\frac{1}{12}) \\]\n",
      "\n",
      "Before adding these fractions, let's find a common denominator. The least common multiple (LCM) of 6, 4, and 12 is 12.\n",
      "\n",
      "\\[ \\text{Combined Rate} = \\frac{2}{12} + \\frac{3}{12} + \\frac{1}{12} \\]\n",
      "\\[ \\text{Combined Rate} = \\frac{2 + 3 + 1}{12} \\]\n",
      "\\[ \\text{Combined Rate} = \\frac{6}{12} \\]\n",
      "\\[ \\text{Combined Rate} = \\frac{1}{2} \\text{ pool per hour} \\]\n",
      "\n",
      "### Step 3: Simplify the combined rate (if necessary)\n",
      "\n",
      "In this case, the combined rate is already simplified:\n",
      "\\[ \\\n",
      "\n",
      "================================================================================\n",
      "DRAFT 3 (Revision 2)\n",
      "================================================================================\n",
      "**Solving the Problem: Filling a Swimming Pool with Multiple Pipes and a Drain**\n",
      "\n",
      "To determine how long it will take to fill the pool when all three pipes (Pipe A, Pipe B) and the drain are opened simultaneously, we need to calculate their combined rate of filling or emptying.\n",
      "\n",
      "### Step 1: Calculate the Rate of Filling for Each Pipe and the Drain\n",
      "\n",
      "Let's start by finding the individual rates at which each pipe fills or empties the pool:\n",
      "\n",
      "- **Pipe A**: Fills the pool in 6 hours. Its rate is:\n",
      "\\[ \\text{Rate of Pipe A} = \\frac{1}{6} \\text{ pool per hour} \\]\n",
      "\n",
      "- **Pipe B**: Fills the pool in 4 hours. Its rate is:\n",
      "\\[ \\text{Rate of Pipe B} = \\frac{1}{4} \\text{ pool per hour} \\]\n",
      "\n",
      "- **The Drain**: Empties the pool in 12 hours. Its rate is:\n",
      "\\[ \\text{Rate of Drain} = -\\frac{1}{12} \\text{ pool per hour} \\]\n",
      "Note that the negative sign indicates emptying.\n",
      "\n",
      "### Step 2: Calculate the Combined Rate\n",
      "\n",
      "To find the combined rate, we add the rates of Pipe A and Pipe B to the rate of the drain. However, because the drain empties the pool, its rate is subtracted from the sum:\n",
      "\n",
      "\\[ \\text{Combined Rate} = \\text{Rate of Pipe A} + \\text{Rate of Pipe B} - \\text{Rate of Drain} \\]\n",
      "Substituting the values we found earlier:\n",
      "\\[ \\text{Combined Rate} = \\frac{1}{6} + \\frac{1}{4} - (-\\frac{1}{12}) \\]\n",
      "\n",
      "To add these fractions, let's find a common denominator. The least common multiple (LCM) of 6, 4, and 12 is 12.\n",
      "\n",
      "\\[ \\text{Combined Rate} = \\frac{2}{12} + \\frac{3}{12} + \\frac{1}{12} \\]\n",
      "\n",
      "Adding the numerators:\n",
      "\\[ \\text{Combined Rate} = \\frac{2 + 3 + 1}{12} = \\frac{6}{12} \\]\n",
      "\n",
      "Simplifying the fraction:\n",
      "\\[ \\text{Combined Rate} = \\frac{6}{12} = \\frac{1}{2\n",
      "\n",
      "================================================================================\n",
      "FINAL ANSWER\n",
      "================================================================================\n",
      "**Solving the Problem: Filling a Swimming Pool with Multiple Pipes and a Drain**\n",
      "\n",
      "To determine how long it will take to fill the pool when all three pipes (Pipe A, Pipe B) and the drain are opened simultaneously, we need to calculate their combined rate of filling or emptying.\n",
      "\n",
      "### Step 1: Calculate the Rate of Filling for Each Pipe and the Drain\n",
      "\n",
      "Let's start by finding the individual rates at which each pipe fills or empties the pool:\n",
      "\n",
      "- **Pipe A**: Fills the pool in 6 hours. Its rate is:\n",
      "\\[ \\text{Rate of Pipe A} = \\frac{1}{6} \\text{ pool per hour} \\]\n",
      "\n",
      "- **Pipe B**: Fills the pool in 4 hours. Its rate is:\n",
      "\\[ \\text{Rate of Pipe B} = \\frac{1}{4} \\text{ pool per hour} \\]\n",
      "\n",
      "- **The Drain**: Empties the pool in 12 hours. Its rate is:\n",
      "\\[ \\text{Rate of Drain} = -\\frac{1}{12} \\text{ pool per hour} \\]\n",
      "Note that the negative sign indicates emptying.\n",
      "\n",
      "### Step 2: Calculate the Combined Rate\n",
      "\n",
      "To find the combined rate, we add the rates of Pipe A and Pipe B to the rate of the drain. However, because the drain empties the pool, its rate is subtracted from the sum:\n",
      "\n",
      "\\[ \\text{Combined Rate} = \\text{Rate of Pipe A} + \\text{Rate of Pipe B} - \\text{Rate of Drain} \\]\n",
      "Substituting the values we found earlier:\n",
      "\\[ \\text{Combined Rate} = \\frac{1}{6} + \\frac{1}{4} - (-\\frac{1}{12}) \\]\n",
      "\n",
      "To add these fractions, let's find a common denominator. The least common multiple (LCM) of 6, 4, and 12 is 12.\n",
      "\n",
      "\\[ \\text{Combined Rate} = \\frac{2}{12} + \\frac{3}{12} + \\frac{1}{12} \\]\n",
      "\n",
      "Adding the numerators:\n",
      "\\[ \\text{Combined Rate} = \\frac{2 + 3 + 1}{12} = \\frac{6}{12} \\]\n",
      "\n",
      "Simplifying the fraction:\n",
      "\\[ \\text{Combined Rate} = \\frac{6}{12} = \\frac{1}{2\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS: Why Sequential Revision Can Fail\n",
      "================================================================================\n",
      "\n",
      "Issues observed:\n",
      "1. Model may confuse itself through vague critique prompts\n",
      "2. No external verification or grounding\n",
      "3. Can \"improve\" away from correct answer\n",
      "4. Temperature=0.7 adds randomness to revisions\n",
      "\n",
      "Better approaches:\n",
      "- Use self-consistency BEFORE revision (get consensus first)\n",
      "- Add explicit verification steps (check arithmetic)\n",
      "- Use lower temperature for revisions (0.1-0.3)\n",
      "- Stop early if answer is already correct\n",
      "- Add constraints (e.g., \"verify all calculations\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://host.docker.internal:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"Extract numeric answer from text\"\"\"\n",
    "    # Look for \"X hours\" pattern\n",
    "    match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*hours?', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"Not found\"\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    \n",
    "    # Step 1: Generate initial draft\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DRAFT 1 (Initial Answer)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    prompt = f\"{question}\\n\\nProvide a detailed answer with step-by-step reasoning.\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    current_draft = response.choices[0].message.content\n",
    "    print(current_draft)\n",
    "    print(f\"\\nExtracted answer: {extract_answer(current_draft)} hours\")\n",
    "    \n",
    "    # Step 2: Iteratively revise the draft\n",
    "    for step in range(2, max_steps + 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"DRAFT {step} (Revision {step-1})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        revision_prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Previous answer:\n",
    "{current_draft}\n",
    "\n",
    "Please review the previous answer and provide an improved version that:\n",
    "1. Fixes any errors or inaccuracies\n",
    "2. Adds more clarity and detail\n",
    "3. Ensures the reasoning is correct\n",
    "\n",
    "Provide your revised answer:\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that carefully reviews and improves answers.\"},\n",
    "                {\"role\": \"user\", \"content\": revision_prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        current_draft = response.choices[0].message.content\n",
    "        print(current_draft)\n",
    "        # print(f\"\\nExtracted answer: {extract_answer(current_draft)} hours\")\n",
    "    \n",
    "    # Step 4: Return the final improved draft\n",
    "    return current_draft\n",
    "\n",
    "\n",
    "# Test with the pool filling question\n",
    "question = \"\"\"A swimming pool can be filled by Pipe A in 6 hours and by Pipe B in 4 hours. However, there's a drain that can empty the full pool in 12 hours. If all three (both pipes and the drain) are opened simultaneously when the pool is empty, how long will it take to fill the pool?\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SEQUENTIAL REVISION TEST\")\n",
    "print(\"=\"*80)\n",
    "print(\"Expected answer: 3 hours\")\n",
    "print(\"Correct reasoning: 1/6 + 1/4 - 1/12 = 2/12 + 3/12 - 1/12 = 4/12 = 1/3 pool/hour → 3 hours\")\n",
    "\n",
    "final_answer = sequential_revision(question, max_steps=3)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL ANSWER\")\n",
    "print(f\"{'='*80}\")\n",
    "print(final_answer)\n",
    "# print(f\"\\nFinal extracted answer: {extract_answer(final_answer)} hours\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS: Why Sequential Revision Can Fail\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Issues observed:\n",
    "1. Model may confuse itself through vague critique prompts\n",
    "2. No external verification or grounding\n",
    "3. Can \"improve\" away from correct answer\n",
    "4. Temperature=0.7 adds randomness to revisions\n",
    "\n",
    "Better approaches:\n",
    "- Use self-consistency BEFORE revision (get consensus first)\n",
    "- Add explicit verification steps (check arithmetic)\n",
    "- Use lower temperature for revisions (0.1-0.3)\n",
    "- Stop early if answer is already correct\n",
    "- Add constraints (e.g., \"verify all calculations\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 2.5 Tree‑of‑Thoughts\n",
    "Tree-of-Thoughts reframes reasoning as a search process rather than a single forward chain.\n",
    "Instead of producing one linear sequence of thoughts, the model generates multiple candidate thoughts at each step, evaluates their promise, and then expands only the best few. This allows exploration of different reasoning paths before committing to a final answer, similar to how humans brainstorm, prune, and refine ideas.\n",
    "\n",
    "\n",
    "In this section, you’ll experiment with two simplified versions of ToT:\n",
    "1. Word Ladder puzzle solver: a small example where each “thought” is a candidate word transition.\n",
    "2. Generic ToT search (depth 2, width 2): a minimal logic to expand, evaluate, and select reasoning branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d047801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hit', 'hot', 'dot', 'dog', 'cog']\n"
     ]
    }
   ],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    # Generate all valid one-letter mutations of 'word' that exist in 'vocabulary' and return them.\n",
    "    result = []\n",
    "    for i in range(len(word)):\n",
    "        for c in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            if c != word[i]:\n",
    "                candidate = word[:i] + c + word[i+1:]\n",
    "                if candidate in vocabulary:\n",
    "                    result.append(candidate)\n",
    "    return result\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    # Search over partial thoughts (paths) using a small beam.\n",
    "    # Step 1: Initialize the frontier with a single path [start]\n",
    "    frontier = [[start]]\n",
    "    \n",
    "    # Step 2: For each depth, expand each path by one neighbor from 'neighbors'\n",
    "    for depth in range(max_depth):\n",
    "        candidates = []\n",
    "        \n",
    "        for path in frontier:\n",
    "            last_word = path[-1]\n",
    "            \n",
    "            # Check if we've reached the goal\n",
    "            if last_word == goal:\n",
    "                return path\n",
    "            \n",
    "            # Expand path with all valid neighbors\n",
    "            for neighbor in neighbors(last_word, vocab):\n",
    "                if neighbor not in path:  # Avoid cycles\n",
    "                    candidates.append(path + [neighbor])\n",
    "        \n",
    "        if not candidates:\n",
    "            return None  # No valid paths found\n",
    "        \n",
    "        # Step 3: Score paths by edit distance between last word and 'goal' (smaller is better)\n",
    "        def edit_distance(w1, w2):\n",
    "            return sum(c1 != c2 for c1, c2 in zip(w1, w2))\n",
    "        \n",
    "        scored_candidates = [(edit_distance(path[-1], goal), path) for path in candidates]\n",
    "        \n",
    "        # Step 4: Keep the top 'beam_width' paths (sort by score ascending)\n",
    "        scored_candidates.sort(key=lambda x: x[0])\n",
    "        frontier = [path for score, path in scored_candidates[:beam_width]]\n",
    "    \n",
    "    # Step 5: Return the best path or None\n",
    "    return frontier[0] if frontier else None\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "print(tree_of_thought(\"hit\", \"cog\", vocab)) # one candidate solution: ['hit', 'hot', 'dot', 'dog', 'cog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89067302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Depth 1/2\n",
      "============================================================\n",
      "\n",
      "Thought: The first step to designing a plan for a weekend science workshop for 12-year-ol...\n",
      "Score: 8/10\n",
      "\n",
      "Top 2 states kept:\n",
      "1. Score 8/10: The first step to designing a plan for a weekend science workshop for 12-year-olds would be to deter...\n",
      "\n",
      "============================================================\n",
      "Depth 2/2\n",
      "============================================================\n",
      "\n",
      "Thought: The next step would be to identify a target group's needs and abilities by condu...\n",
      "Score: 8/10\n",
      "\n",
      "Top 2 states kept:\n",
      "1. Score 8/10: The first step to designing a plan for a weekend science workshop for 12-year-olds would be to deter...\n",
      "\n",
      "============================================================\n",
      "FINAL BEST SOLUTION\n",
      "============================================================\n",
      "Score: 8/10\n",
      "\n",
      "The first step to designing a plan for a weekend science workshop for 12-year-olds would be to determine the specific theme or topic(s) of interest that will guide the content and activities, such as space exploration, environmental science, or chemistry experiments.\n",
      "\n",
      "This step involves deciding what type of hands-on learning experiences, lectures, and discussions will engage participants and align with their interests.\n",
      "\n",
      "The next step would be to identify a target group's needs and abilities by conducting a survey or gathering information from potential parents/guardians to understand the level of prior knowledge, interest, and comfort with hands-on activities for 12-year-old participants. This will help tailor the content and activities to meet their expectations and learning styles.\n"
     ]
    }
   ],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://host.docker.internal:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    # Propose up to k next \"thoughts\" that extend the current partial solution/state.\n",
    "    # Steps: build a short prompt with problem + current state; call your client with n=k. Then return a list of stripped strings (≤ k).\n",
    "    \n",
    "    if state == \"\":\n",
    "        prompt = f\"Question: {question}\\n\\nPropose the first step to solve this problem. Be concise (1-2 sentences).\"\n",
    "    else:\n",
    "        prompt = f\"Question: {question}\\n\\nCurrent progress:\\n{state}\\n\\nPropose the next step to continue solving this. Be concise (1-2 sentences).\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.8,\n",
    "        max_tokens=100,\n",
    "        n=k  # Generate k different thoughts\n",
    "    )\n",
    "    \n",
    "    thoughts = [choice.message.content.strip() for choice in response.choices]\n",
    "    return thoughts\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    # Score how promising a partial solution is on a 1–10 scale (higher is better).\n",
    "    # Steps: build a rating prompt; call the model; parse the first integer 1–10;\n",
    "    \n",
    "    prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Current partial solution:\n",
    "{state}\n",
    "\n",
    "Rate how promising this partial solution is on a scale of 1-10, where:\n",
    "- 1-3: Poor progress, unlikely to lead to a good solution\n",
    "- 4-6: Decent progress, some potential\n",
    "- 7-9: Good progress, likely to lead to a strong solution\n",
    "- 10: Excellent, nearly complete solution\n",
    "\n",
    "Respond with ONLY a number from 1-10.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    \n",
    "    # Parse the score from response\n",
    "    text = response.choices[0].message.content.strip()\n",
    "    match = re.search(r'\\b([1-9]|10)\\b', text)\n",
    "    score = int(match.group(1)) if match else 5  # Default to 5 if parsing fails\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    # Run a tiny ToT search: expand states with propose_thoughts, score with score_state, keep top-k at each depth.\n",
    "    # Steps: initialize frontier=[(\"\", 0)]; for each depth, expand each state with k=width thoughts; score each; sort by score desc; keep top 'width'; return best state and score.\n",
    "    \n",
    "    # Initialize frontier with empty state\n",
    "    frontier = [(\"\", 0)]  # (state, score)\n",
    "    \n",
    "    for d in range(depth):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Depth {d+1}/{depth}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        candidates = []\n",
    "        \n",
    "        # Expand each state in frontier\n",
    "        for state, _ in frontier:\n",
    "            # Generate width new thoughts from this state\n",
    "            thoughts = propose_thoughts(question, state, k=width)\n",
    "            \n",
    "            for thought in thoughts:\n",
    "                # Build new state by appending thought\n",
    "                if state == \"\":\n",
    "                    new_state = thought\n",
    "                else:\n",
    "                    new_state = f\"{state}\\n\\n{thought}\"\n",
    "                \n",
    "                # Score the new state\n",
    "                score = score_state(question, new_state)\n",
    "                candidates.append((new_state, score))\n",
    "                \n",
    "                print(f\"\\nThought: {thought[:80]}...\")\n",
    "                print(f\"Score: {score}/10\")\n",
    "        \n",
    "        # Keep only top 'width' candidates\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        frontier = candidates[:width]\n",
    "        \n",
    "        print(f\"\\nTop {width} states kept:\")\n",
    "        for i, (s, sc) in enumerate(frontier, 1):\n",
    "            print(f\"{i}. Score {sc}/10: {s[:100]}...\")\n",
    "    \n",
    "    # Return the best state and its score\n",
    "    best_state, best_score = frontier[0]\n",
    "    return best_state, best_score\n",
    "\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question, depth=2, width=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL BEST SOLUTION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Score: {score}/10\\n\")\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‑ Training Models for Reasoning\n",
    "\n",
    "### 3.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 3.2: ORM vs PRM + RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‑tuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‑out (actions + log‑probs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 4‑ A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model (e.g., deepseek-r1) with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with a reasoning model (e.g., `deepseek-r1`) in a multi-step setup. We follow the *ReAct* pattern (reason → tool → observation):\n",
    "\n",
    "1. The model reasoins and decides to use tools\n",
    "2. The agent searches and feed condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `AgentType.OPENAI_FUNCTIONS`, which hides the loop inside the LangChain agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    # Use DDGS to run a simple web search and return joined snippets.\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(query, max_results=k))\n",
    "    \n",
    "    # Extract and join snippets from search results\n",
    "    snippets = [f\"[{r['title']}] {r['body']}\" for r in results]\n",
    "    return \"\\n\\n\".join(snippets)\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"DuckDuckGo Search\",\n",
    "    func=ddg_search,\n",
    "    description=\"Search the public web. Input: a plain English query. Returns: concatenated snippets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1439/3397914316.py:9: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=MODEL, base_url=\"http://host.docker.internal:11434\")\n",
      "/tmp/ipykernel_1439/3397914316.py:12: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent([search_tool], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
      "/tmp/ipykernel_1439/3397914316.py:15: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = agent.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the best resources to learn machine learning in 2025, I need to search for relevant information on the web.\n",
      "\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"best machine learning resources 2025\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[7 Best Machine Learning Courses for 2025 (read this first) – LearnDataSci] If you take Andrew Ng’s Machine Learning course, which uses Octave, you should learn Python either during the course or after since you’ll need it eventually. Additionally, another excellent Python resource is dataquest.io, which has many free Python lessons in their interactive browser environment.\n",
      "\n",
      "[8 Best Machine Learning Software To Use in 2025 | Anaconda] July 16, 2025 - Keras is best for quickly prototyping and experimenting with neural network architectures. Getting familiar with machine learning software can seem daunting, but there are many resources that can help you get started and progress in your journey.\n",
      "\n",
      "[Machine Learning in 2025: What learning resources have helped you most, and what are you looking forward to learning for the future? : r/learnmachinelearning] December 14, 2024 - Welcome to r/learnmachinelearning - a community of learners and educators passionate about machine learning! This is your space to ask questions, share resources, and grow together in understanding ML concepts - from basic principles to advanced techniques. Whether you're writing your first neural network or diving into transformers, you'll find supportive peers here. For ML research, /r/machinelearning For resume review, /r/engineeringresumes For ML engineers, /r/mlengineering ... Accessibility Reddit, Inc. © 2025.\n",
      "\n",
      "[50+ Machine Learning Resources for Self Study in 2025] March 3, 2025 - Explore top-notch 50+ Machine Learning resources for self-study in 2025. From courses to tutorials, find everything you need today!\n",
      "\n",
      "[The Roadmap for Mastering Machine Learning in 2025 - MachineLearningMastery.com] April 20, 2025 - PyTorch for Deep Learning & Machine Learning – Full Course – YouTube ... TensorFlow Tutorial 1 – Installation and Setup Deep Learning Environment (Anaconda and PyCharm ) (Recommended) TensorFlow 2.0 Complete Course – Python Neural Networks ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are the best resources to learn machine learning in 2025?\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"best machine learning resources 2025\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[The best free AI courses and certificates for upskilling in 2025... | ZDNET] I learned a lot in this course, especially about how to collect and prepare data for machinelearning. I also was able to stretch my Python programming knowledge, specifically about how a machinelearning model can be built in Python.\n",
      "\n",
      "[Best Artificial Intelligence Software: User Reviews from October 2025] Allow users to create machinelearning algorithms and/or offer pre-built machinelearning algorithms for more novice users to build applications. Present a way for developers to connect data to the algorithms for them to learn and adapt. What is AI software?\n",
      "\n",
      "[Best Python libraries for MachineLearning - GeeksforGeeks] Here’s a list of some of the best Python libraries for MachineLearning that streamline development\n",
      "\n",
      "[GitHub - VenkataAnilKumar/AI-ML-Engineering: AI-ML-Engineer-Hub...] A clean, visual, and beginner-friendly hub of free and open-source AI/ML learningresources across foundations, machinelearning, deep learning, tools, and practical projects. Last checked: 2025-10-29.\n",
      "\n",
      "[Top Python AI and MachineLearning Libraries - TechRepublic] Learn about some of the best Python libraries for programming Artificial Intelligence (AI), MachineLearning (ML), and Deep Learning (DL).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are the best resources to learn machine learning in 2025?\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"best machine learning resources 2025\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[7 Best Machine Learning Courses for 2025 (read this first) – LearnDataSci] If you take Andrew Ng’s Machine Learning course, which uses Octave, you should learn Python either during the course or after since you’ll need it eventually. Additionally, another excellent Python resource is dataquest.io, which has many free Python lessons in their interactive browser environment.\n",
      "\n",
      "[8 Best Machine Learning Software To Use in 2025 | Anaconda] July 16, 2025 - Keras is best for quickly prototyping and experimenting with neural network architectures. Getting familiar with machine learning software can seem daunting, but there are many resources that can help you get started and progress in your journey.\n",
      "\n",
      "[Machine Learning in 2025: What learning resources have helped you most, and what are you looking forward to learning for the future? : r/learnmachinelearning] December 14, 2024 - Welcome to r/learnmachinelearning - a community of learners and educators passionate about machine learning! This is your space to ask questions, share resources, and grow together in understanding ML concepts - from basic principles to advanced techniques. Whether you're writing your first neural network or diving into transformers, you'll find supportive peers here. For ML research, /r/machinelearning For resume review, /r/engineeringresumes For ML engineers, /r/mlengineering ... Accessibility Reddit, Inc. © 2025.\n",
      "\n",
      "[50+ Machine Learning Resources for Self Study in 2025] March 3, 2025 - Explore top-notch 50+ Machine Learning resources for self-study in 2025. From courses to tutorials, find everything you need today!\n",
      "\n",
      "[The Roadmap for Mastering Machine Learning in 2025 - MachineLearningMastery.com] April 20, 2025 - PyTorch for Deep Learning & Machine Learning – Full Course – YouTube ... TensorFlow Tutorial 1 – Installation and Setup Deep Learning Environment (Anaconda and PyCharm ) (Recommended) TensorFlow 2.0 Complete Course – Python Neural Networks ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are the best resources to learn machine learning in 2025?\n",
      "Thought: I now know the final answer\n",
      "Final Answer: Some of the best resources to learn machine learning in 2025 include:\n",
      "\n",
      "* LearnDataSci's top 7 machine learning courses for 2025, which include Andrew Ng's Machine Learning course and dataquest.io.\n",
      "* Anaconda's list of 8 best machine learning software to use in 2025, including Keras, which is great for quickly prototyping and experimenting with neural network architectures.\n",
      "* The r/learnmachinelearning community on Reddit, which provides a supportive space for learners and educators to share resources and grow together in understanding ML concepts.\n",
      "* MachineLearningMastery.com's roadmap for mastering machine learning in 2025, which includes courses and tutorials on PyTorch and TensorFlow.\n",
      "* Data science courses and tutorials on platforms like GitHub, GeeksforGeeks, and TechRepublic.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "================================================================================\n",
      "FINAL ANSWER\n",
      "================================================================================\n",
      "Some of the best resources to learn machine learning in 2025 include:\n",
      "\n",
      "* LearnDataSci's top 7 machine learning courses for 2025, which include Andrew Ng's Machine Learning course and dataquest.io.\n",
      "* Anaconda's list of 8 best machine learning software to use in 2025, including Keras, which is great for quickly prototyping and experimenting with neural network architectures.\n",
      "* The r/learnmachinelearning community on Reddit, which provides a supportive space for learners and educators to share resources and grow together in understanding ML concepts.\n",
      "* MachineLearningMastery.com's roadmap for mastering machine learning in 2025, which includes courses and tutorials on PyTorch and TensorFlow.\n",
      "* Data science courses and tutorials on platforms like GitHub, GeeksforGeeks, and TechRepublic.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "#MODEL = \"deepseek-r1:8b\"\n",
    "MODEL = \"llama3.2:3b\"\n",
    "question = \"What are the best resources to learn machine learning in 2025?\"\n",
    "\n",
    "# Step 1: Initialize the reasoning model via ChatOllama\n",
    "llm = ChatOllama(model=MODEL, base_url=\"http://host.docker.internal:11434\")\n",
    "\n",
    "# Step 2: Build the agent with tool access (DuckDuckGo Search) and function-calling interface (initialize_agent)\n",
    "agent = initialize_agent([search_tool], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Step 3: Ask a query and let the agent search + reason to produce an answer\n",
    "result = agent.run(question)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# Optional (Multi-agent Deep Research)\n",
    "Instead of a single multi-step agent, you can design multiple collaborating agents such as a Planner, Searcher, Summarizer, and Verifier that pass information and refine each other’s outputs. This setup improves robustness, diversity of reasoning, and division of labor.\n",
    "\n",
    "Try building a simple setup with 2–3 agents that share goals and messages, for example Planner → Researcher → Writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_research(query, n=3):\n",
    "    # Run n independent research runs in parallel and return their answers.\n",
    "    # Steps: use ThreadPoolExecutor; submit n calls to your agent/search pipeline; gather results in order.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "answers = parallel_research(\"What are the best resources to learn ML in 2025?\")\n",
    "for i,a in enumerate(answers,1):\n",
    "    print(f\"[Run {i}] {a[:200]}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "* Practised various inference‑time reasoning methods\n",
    "* Gained intuition about training reasoning models\n",
    "* You have built a **deep-research agent**: reasoning model like deep-seek r1 + ReAct-style agent + tool use (web search)\n",
    "* Try adding more tools, and extending the deep-research to a multi-agent system: many agents researching web in parallel.\n",
    "\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
